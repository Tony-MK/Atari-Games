{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Actions: 9\n",
      "Observation Space: (128,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from uuid import uuid3\n",
    "import time\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,BatchNormalization,Conv1D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "ENV_NAME = \"BeamRider-ram-v0\"\n",
    "\n",
    "env = gym.make(ENV_NAME);\n",
    "Number_of_Actions = env.action_space.n\n",
    "Observation_Shape = env.observation_space.shape\n",
    "memory = deque(maxlen=1024)\n",
    "print(\"Number of Actions: {}\\nObservation Space: {}\".format(Number_of_Actions,Observation_Shape));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_INDEX = 0\n",
    "ACT_INDEX = 1\n",
    "XOBS_INDEX = 2\n",
    "REWARD_INDEX = 3\n",
    "DONE_INDEX = 4\n",
    "INFO_INDEX = 5\n",
    "\n",
    "\n",
    "DENSE_MODEL = 0\n",
    "CONV_MODEL = 1\n",
    "\n",
    "AtariDatabase = MongoClient(\"localhost\")[\"AtariAgents\"]\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self,model=None,model_type=None,loss=\"mse\",optimizer=\"adam\",metrics=[\"mse\",\"mae\"],\n",
    "                 batch_size =1024,number_of_memories_to_replay = 10,gamma=0.99,eps_init=0.9,eps_decay=0.99,\n",
    "                 eps_min=0.05,max_timesteps=10000,render_game=True):\n",
    "        \n",
    "        \"\"\"\n",
    "            Eposide_Experience = An Array holding the memory of the eposide;\n",
    "            L          =        Lenght of  Eposide_Experience \n",
    "            N          =        batch_size or self.BatchSize\n",
    "\n",
    "            While playing, After N Timesteps the Agent will train from Eposide_Experience[L: N],\n",
    "        \"\"\"\n",
    "        self.MemoryCollection = AtariDatabase[\"Memory_\"+ENV_NAME]\n",
    "        self.AgentsCollection = AtariDatabase[\"Agents_\"+ENV_NAME]\n",
    "        self.Id = uuid4()\n",
    "        #Agent's Hyper-Parameters\n",
    "        self.Loss = loss\n",
    "        self.Optimizer = optimizer\n",
    "        self.BatchSize = batch_size;\n",
    "        self.Number_Of_Memories_To_Replay = number_of_memories_to_replay;\n",
    "        self.MaxTimeSteps = max_timesteps;\n",
    "        \n",
    "        #Agent Bellman Paramenters\n",
    "        self.Epilison = eps_init;\n",
    "        self.Epilison_Decay = eps_decay;\n",
    "        self.Epilison_MinLimit = eps_min;\n",
    "        self.Gamma = gamma;\n",
    "        \n",
    "        \n",
    "        # Agent's Settings;\n",
    "        self.Experience = []\n",
    "        self.HighScore = 0;\n",
    "        self.RenderGame = render_game\n",
    "        self.Model_Type = model_type\n",
    "        self.NumberofActions = Number_of_Actions;\n",
    "        \n",
    "        # Agent's History\n",
    "        self.ActionsHistory = []\n",
    "        self.RewardsHistory = []\n",
    "        self.Metrics = list(set(metrics))\n",
    "        self.Metrics.remove(self.Loss);\n",
    "        self.MetricsHistory = {metric:[] for metric in self.Metrics}\n",
    "        self.MetricsHistory[self.Loss] = []\n",
    "        \n",
    "       # Agent's Model (The Brain);\n",
    "        self.Model = model\n",
    "        if self.Model == None:\n",
    "            self.createDenseNet();\n",
    "        elif self.Model == DENSE_MODEL:\n",
    "            self.createDenseNet();\n",
    "        elif self.Model == CONV_MODEL:\n",
    "            self.createConvNet();\n",
    "            \n",
    "            \n",
    "        self.formatObservation = lambda obs: np.expand_dims(np.array(obs,dtype=\"float\").reshape(self.Input_Shape)/255.0,axis=0);\n",
    "        self.obsToImg = lambda obs:  np.array(obs,dtype=\"float\").reshape(16,8,1)/255.0\n",
    "        self.updateAgent =  lambda: self.AgentsCollection.update_one(\n",
    "            filter={\"_id\":str(self.Id)},\n",
    "            update={\"$push\":{\"weights\":agent.Model.get_weights()},\n",
    "                    \"$set\":{\"metrics\":self.MetricsHistory,\"rewards\":self.RewardsHistory}\n",
    "                   })\n",
    "        \n",
    "        # String Parsers\n",
    "        self.parseScore = lambda: \"Score: \"+str(self.getCurrentScore())+\" |  HighScore: \"+str(self.HighScore);\n",
    "        self.parseAverageCost = lambda _index: \"Cost: \"+\" | \".join([\"%s:%.5f\"%(metric[0].upper()+metric[1:],sum(self.MetricsHistory[metric][_index])/len(self.MetricsHistory[metric][_index])) for metric in self.MetricsHistory])\n",
    "        self.parseStatus =  lambda: \"%d# | %s | ~ | %s | ~ | %s | ~ | Eps: %.3f Lives Left: %d \\t\"%(self.TimeStep,self.parseAverageCost(-1),self.parseActions(),self.parseScore(),self.Epilison,self.Experience[-1][INFO_INDEX])\n",
    "        pass;\n",
    "    \n",
    "    def createDenseNet(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Four Dense Layers - Input Shape \n",
    "        \"\"\"\n",
    "        self.Input_Shape = (1,128);self.Model_Type = DENSE_MODEL;\n",
    "        \n",
    "        self.Model = Sequential();\n",
    "        self.Model.add(Dense(256,use_bias=True,input_shape=self.Input_Shape));\n",
    "        self.Model.add(Activation(\"relu\"));\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.Model.add(Dense(1024,use_bias=True));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"))\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.finalizeModel(\"3 Layers Dense\");\n",
    "    \n",
    "    def createConvNet(self,):\n",
    "        \"\"\"\n",
    "            A Convoluational Neural Network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Input_Shape = (8,16);self.Model_Type = CONV_MODEL;\n",
    "                \n",
    "        self.Model = Sequential();\n",
    "        self.Model.add(Conv1D(filters=8,kernel_size=(4),use_bias=True,input_shape=(self.Input_Shape)));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"));\n",
    "        self.Model.add(Dropout(0.01));\n",
    "        \n",
    "        self.Model.add(Conv1D(filters=24,kernel_size=(1),use_bias=True));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"));\n",
    "        self.Model.add(Dropout(0.01));\n",
    "        self.Model.add(Flatten());\n",
    "        self.Model.add(BatchNormalization())\n",
    "\n",
    "        \n",
    "        self.finalizeModel(\"2 Convoluational Layers and 1 Dense Layers\");\n",
    "    \n",
    "    def finalizeModel(self,structure_info):\n",
    "        \n",
    "        self.Model.add(Dense(self.NumberofActions,use_bias=True,));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"linear\"));\n",
    "        self.Model.compile(loss=self.Loss,optimizer=self.Optimizer,metrics=self.Metrics);\n",
    "        self.AgentsCollection.insert_one({\"_id\":str(self.Id),\"config\":self.Model.to_json(),\"weights\":[]});\n",
    "        print(\"Succesfully Created a Neural Network with {} .It has  Input Shape of {}\".format(structure_info,self.Input_Shape),end=\"\\r\")\n",
    "\n",
    "        \n",
    "    def predictAction(self,obs):        \n",
    "        return np.argmax(self.Model.predict(obs))\n",
    "        \n",
    "    def getAction(self,obs):\n",
    "        \"\"\" Choose Random Action if random number is smaller than epilision for exporation \"\"\"\n",
    "        if np.random.uniform(0,1) > self.Epilison :\n",
    "            return self.predictAction(obs);\n",
    "        return np.random.choice(self.NumberofActions); \n",
    "    \n",
    "    def learn(self,experience):\n",
    "        if len(experience) == 0 :\n",
    "            return np.nan\n",
    "        x = np.array([exp[0] for exp in experience]).reshape((len(experience),)+self.Input_Shape)\n",
    "        y = self.Model.predict(x)\n",
    "        for i in range(len(experience)):\n",
    "            #Bellman Equation\n",
    "            if experience[i][4]:\n",
    "                y[i:experience[i][1]] = experience[i][3];\n",
    "            elif len(experience)-1 == i:\n",
    "                y[i:experience[i][1]] = experience[i][3] + self.Gamma * np.max(experience[len(experience)-1][2])\n",
    "            else:\n",
    "                y[i:experience[i][1]] = experience[i][3] + self.Gamma * np.max(y[i+1]);\n",
    "            \n",
    "\n",
    "        for metric,cost in zip(self.MetricsHistory,self.Model.train_on_batch(x,y)):\n",
    "            self.MetricsHistory[metric][-1].append(cost)\n",
    "        print(self.parseStatus(),end=\"\\r\");\n",
    "    \n",
    "    def restartGame(self,):\n",
    "        self.TimeStep = 0;\n",
    "        for metric in self.MetricsHistory:\n",
    "            self.MetricsHistory[metric].append([])\n",
    "        self.RewardsHistory.append([[]])\n",
    "        self.ActionsHistory.append({act:0 for act in range(self.NumberofActions)});\n",
    "        return self.formatObservation(env.reset())\n",
    "    \n",
    "    def getCurrentScore(self):\n",
    "        score = sum(self.RewardsHistory[-1][-1]);\n",
    "        if score > self.HighScore:\n",
    "            self.HighScore = score;\n",
    "        return score\n",
    "    \n",
    "    def parseActions(self,nth_experience=-1):\n",
    "        total = sum([act for act in self.ActionsHistory[nth_experience].values()]);\n",
    "        return \"Actions: \"+\" | \".join([\"%.1f%s\"%(100*(value/total),\"%\") for value in self.ActionsHistory[nth_experience].values()]);\n",
    "        \n",
    "    def render(self,):\n",
    "        if self.RenderGame: \n",
    "            env.render(); # Rendering Frame(s)\n",
    "            \n",
    "    def playGame(self):\n",
    "        obs = self.restartGame();\n",
    "        self.render()\n",
    "        prev_score = 0\n",
    "        while True:\n",
    "            \n",
    "            # Pushing the new Observation to for new State \n",
    "            self.Experience.append([obs])\n",
    "            \n",
    "            #Predicting then Sending Action to the enviroment\n",
    "            self.Experience[-1].append(self.getAction(self.Experience[-1][OBS_INDEX]));\n",
    "            for item in env.step(self.Experience[-1][ACT_INDEX]):\n",
    "                self.Experience[-1].append(item);\n",
    "            self.Experience[-1][INFO_INDEX] = self.Experience[-1][INFO_INDEX][\"ale.lives\"]\n",
    "            self.Experience[-1][XOBS_INDEX] = self.formatObservation(self.Experience[-1][XOBS_INDEX]);\n",
    "            self.RewardsHistory[-1][-1].append(self.Experience[-1][REWARD_INDEX]);\n",
    "            self.ActionsHistory[-1][self.Experience[-1][ACT_INDEX]] += 1;\n",
    "            self.TimeStep +=1;\n",
    "            self.render();\n",
    "            \n",
    "            # If agent is killed OR timestep divide by self.BatchSize has no reminader\n",
    "            if self.Experience[-1][DONE_INDEX] or self.TimeStep%self.BatchSize == 0:\n",
    "                \n",
    "                # Training on Experience from the Current (self.Experience) and Previous Game (memory).\n",
    "                self.learn(self.getExperienciesFromMemory(self.Number_Of_Memories_To_Replay,memories=self.Experience[self.TimeStep-self.BatchSize:]));\n",
    "                \n",
    "                # STOP PLAYING : If the agent has no lives or not imporving it's score\n",
    "                score = self.getCurrentScore();\n",
    "                self.RewardsHistory[-1]\n",
    "                if score == prev_score: \n",
    "                    break\n",
    "                elif self.Experience[-1][INFO_INDEX] == 0:\n",
    "                    break;\n",
    "                elif self.Experience[-1][DONE_INDEX]:\n",
    "                    self.RewardsHistory[-1].append([])\n",
    "                    prev_score = 0;\n",
    "                else:\n",
    "                    prev_score = score;\n",
    "                    \n",
    "\n",
    "        # Training on Experience from the Current (self.Experience) and Previous Game (memory).\n",
    "        self.learn(self.getExperienciesFromMemory(self.Number_Of_Memories_To_Replay,memories=self.Experience[self.TimeStep-self.BatchSize:]));\n",
    "        #self.MemoryCollection.insert_one({\"model_type\":self.Model_Type,\"game_experience\":self.Experience,\"agent_id\":str(self.ID)})\n",
    "        memory.append(self.Experience[:-1]);\n",
    "    \n",
    "        # Reducing The Epilison to give agent a bigger probability to choose action and action\n",
    "        if self.Epilison > self.Epilison_MinLimit:\n",
    "            self.Epilison = self.Epilison*self.Epilison_Decay;\n",
    "        pass;\n",
    "    \n",
    "    def getBatchFromExperience(self,experience):\n",
    "        startIndex = max(0,np.random.choice(len(experience))-self.BatchSize);\n",
    "        return experience[startIndex:startIndex+self.BatchSize]\n",
    "    \n",
    "    def getExperienciesFromMemory(self,number_of_memories_to_fecth,memories = []):\n",
    "        if len(memory) > 0:\n",
    "            for _ in range(min(len(memory),number_of_memories_to_fecth)):\n",
    "                memories.extend(self.getBatchFromExperience(memory[np.random.choice(len(memory))]))\n",
    "        return memories\n",
    "        \n",
    "    def pratice(self,trials=1000,testing_interval=3):\n",
    "        np.random.seed(0);\n",
    "        for n_trial in range(0,trials):\n",
    "            print(\"\\nPracting... On Trial %d out of %d Trials\"%(n_trial+1,trials))\n",
    "            self.playGame();\n",
    "            print()\n",
    "            if n_trial%testing_interval == 0:\n",
    "                self.play();\n",
    "            \n",
    "                \n",
    "    \n",
    "    \n",
    "    def play(self,):\n",
    "        eps = self.Epilison;\n",
    "        self.Epilison = 0\n",
    "        print(\"Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\")\n",
    "        self.playGame();\n",
    "        self.Epilison = eps;\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully Created a Neural Network with 2 Convoluational Layers and 1 Dense Layers .It has  Input Shape of (8, 16)\n",
      "Practing... On Trail 1 out of 100000 Trials\n",
      "512# | Cost: Mae:1.00001 | Mse:0.60621 | ~ | Actions: 47.1% | 7.4% | 7.0% | 8.8% | 5.1% | 6.8% | 4.1% | 6.4% | 7.2% | ~ | Score: 44.0 |  HighScore: 44.0 | ~ | Eps: 0.590 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.98553 | Mse:0.59876 | ~ | Actions: 100.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 44.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 2 out of 100000 Trials\n",
      "256# | Cost: Mae:0.97044 | Mse:0.60674 | ~ | Actions: 46.5% | 6.6% | 5.5% | 10.5% | 5.1% | 6.2% | 7.8% | 6.2% | 5.5% | ~ | Score: 0.0 |  HighScore: 44.0 | ~ | Eps: 0.564 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 3 out of 100000 Trials\n",
      "1280# | Cost: Mae:0.94843 | Mse:0.62083 | ~ | Actions: 50.9% | 5.6% | 4.9% | 6.1% | 6.6% | 7.3% | 5.9% | 7.0% | 5.7% | ~ | Score: 308.0 |  HighScore: 308.0 | ~ | Eps: 0.539 Lives Left: 1 \t\n",
      "\n",
      "Practing... On Trail 4 out of 100000 Trials\n",
      "512# | Cost: Mae:0.93591 | Mse:0.61390 | ~ | Actions: 52.9% | 6.4% | 7.2% | 5.3% | 6.4% | 5.7% | 5.9% | 6.1% | 4.1% | ~ | Score: 44.0 |  HighScore: 308.0 | ~ | Eps: 0.515 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "512# | Cost: Mae:0.92419 | Mse:0.62245 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | ~ | Score: 132.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 5 out of 100000 Trials\n",
      "256# | Cost: Mae:0.91473 | Mse:0.61064 | ~ | Actions: 58.6% | 2.0% | 3.5% | 6.6% | 7.8% | 4.7% | 7.4% | 4.7% | 4.7% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.492 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 6 out of 100000 Trials\n",
      "256# | Cost: Mae:0.91183 | Mse:0.59839 | ~ | Actions: 48.4% | 7.4% | 8.6% | 6.6% | 5.5% | 4.7% | 5.5% | 6.6% | 6.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.470 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 7 out of 100000 Trials\n",
      "256# | Cost: Mae:0.91276 | Mse:0.61563 | ~ | Actions: 57.0% | 2.3% | 4.7% | 9.0% | 5.9% | 5.5% | 5.5% | 4.7% | 5.5% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.450 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.90682 | Mse:0.61166 | ~ | Actions: 100.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 8 out of 100000 Trials\n",
      "256# | Cost: Mae:0.90407 | Mse:0.61537 | ~ | Actions: 3.1% | 4.3% | 4.7% | 6.2% | 8.2% | 60.2% | 7.0% | 3.5% | 2.7% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.430 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 9 out of 100000 Trials\n",
      "512# | Cost: Mae:0.90182 | Mse:0.62437 | ~ | Actions: 64.5% | 2.1% | 3.5% | 4.3% | 5.5% | 6.2% | 4.7% | 4.1% | 5.1% | ~ | Score: 44.0 |  HighScore: 308.0 | ~ | Eps: 0.411 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 10 out of 100000 Trials\n",
      "768# | Cost: Mae:0.89772 | Mse:0.62794 | ~ | Actions: 64.8% | 5.3% | 4.3% | 5.1% | 6.1% | 4.0% | 3.4% | 3.4% | 3.5% | ~ | Score: 132.0 |  HighScore: 308.0 | ~ | Eps: 0.392 Lives Left: 2 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.90199 | Mse:0.63307 | ~ | Actions: 100.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 11 out of 100000 Trials\n",
      "768# | Cost: Mae:0.90419 | Mse:0.62819 | ~ | Actions: 3.5% | 3.4% | 4.6% | 3.1% | 4.0% | 3.0% | 4.9% | 69.7% | 3.8% | ~ | Score: 132.0 |  HighScore: 308.0 | ~ | Eps: 0.375 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 12 out of 100000 Trials\n",
      "256# | Cost: Mae:0.89956 | Mse:0.64261 | ~ | Actions: 3.5% | 3.9% | 5.5% | 3.9% | 2.0% | 71.9% | 5.9% | 1.6% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.358 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 13 out of 100000 Trials\n",
      "256# | Cost: Mae:0.89762 | Mse:0.65914 | ~ | Actions: 2.3% | 5.1% | 3.9% | 2.7% | 3.9% | 73.0% | 3.5% | 2.3% | 3.1% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.343 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.89920 | Mse:0.67479 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 14 out of 100000 Trials\n",
      "256# | Cost: Mae:0.89962 | Mse:0.66471 | ~ | Actions: 4.3% | 3.1% | 3.5% | 3.5% | 3.1% | 67.6% | 5.9% | 3.9% | 5.1% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.327 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 15 out of 100000 Trials\n",
      "512# | Cost: Mae:0.90468 | Mse:0.66123 | ~ | Actions: 3.7% | 3.9% | 2.7% | 2.7% | 3.3% | 75.4% | 2.7% | 3.5% | 2.0% | ~ | Score: 44.0 |  HighScore: 308.0 | ~ | Eps: 0.313 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 16 out of 100000 Trials\n",
      "1280# | Cost: Mae:0.90473 | Mse:0.67059 | ~ | Actions: 3.3% | 3.4% | 3.9% | 4.0% | 3.3% | 72.5% | 3.1% | 3.7% | 2.8% | ~ | Score: 176.0 |  HighScore: 308.0 | ~ | Eps: 0.299 Lives Left: 1 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.92016 | Mse:0.67013 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 17 out of 100000 Trials\n",
      "256# | Cost: Mae:0.91017 | Mse:0.68983 | ~ | Actions: 4.3% | 3.1% | 2.7% | 1.6% | 3.5% | 75.0% | 2.7% | 3.1% | 3.9% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.286 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 18 out of 100000 Trials\n",
      "256# | Cost: Mae:0.92067 | Mse:0.67407 | ~ | Actions: 4.7% | 3.5% | 3.1% | 2.0% | 3.1% | 75.0% | 3.1% | 2.3% | 3.1% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.273 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 19 out of 100000 Trials\n",
      "256# | Cost: Mae:0.92025 | Mse:0.69071 | ~ | Actions: 1.6% | 5.1% | 3.9% | 4.7% | 4.3% | 73.8% | 0.4% | 3.5% | 2.7% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.261 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.92396 | Mse:0.69607 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 20 out of 100000 Trials\n",
      "768# | Cost: Mae:0.92310 | Mse:0.70911 | ~ | Actions: 2.5% | 3.3% | 2.3% | 3.0% | 3.6% | 75.1% | 2.7% | 3.9% | 3.5% | ~ | Score: 88.0 |  HighScore: 308.0 | ~ | Eps: 0.249 Lives Left: 2 \t\n",
      "\n",
      "Practing... On Trail 21 out of 100000 Trials\n",
      "256# | Cost: Mae:0.92300 | Mse:0.71298 | ~ | Actions: 0.4% | 3.1% | 2.0% | 3.1% | 2.3% | 81.6% | 0.8% | 2.0% | 4.7% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.238 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 22 out of 100000 Trials\n",
      "256# | Cost: Mae:0.93117 | Mse:0.71470 | ~ | Actions: 3.1% | 1.6% | 1.2% | 2.7% | 3.1% | 82.0% | 2.3% | 1.2% | 2.7% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.228 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.93166 | Mse:0.70842 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 23 out of 100000 Trials\n",
      "256# | Cost: Mae:0.94093 | Mse:0.73272 | ~ | Actions: 2.7% | 1.2% | 2.7% | 3.5% | 3.1% | 79.7% | 2.7% | 1.6% | 2.7% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.218 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 24 out of 100000 Trials\n",
      "256# | Cost: Mae:0.94151 | Mse:0.73695 | ~ | Actions: 2.0% | 2.0% | 1.6% | 2.7% | 2.7% | 83.6% | 2.3% | 2.0% | 1.2% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.208 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 25 out of 100000 Trials\n",
      "256# | Cost: Mae:0.93863 | Mse:0.73307 | ~ | Actions: 3.1% | 0.8% | 2.7% | 2.7% | 3.5% | 80.1% | 2.3% | 2.7% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.199 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256# | Cost: Mae:0.94994 | Mse:0.74199 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 26 out of 100000 Trials\n",
      "256# | Cost: Mae:0.93813 | Mse:0.73391 | ~ | Actions: 2.0% | 2.3% | 2.0% | 3.1% | 2.0% | 82.4% | 2.3% | 2.3% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.190 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 27 out of 100000 Trials\n",
      "256# | Cost: Mae:0.95475 | Mse:0.75964 | ~ | Actions: 3.1% | 1.2% | 3.1% | 3.9% | 1.6% | 80.9% | 2.0% | 2.3% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.182 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 28 out of 100000 Trials\n",
      "256# | Cost: Mae:0.95764 | Mse:0.75236 | ~ | Actions: 2.3% | 3.1% | 3.1% | 0.8% | 3.1% | 82.8% | 1.2% | 2.3% | 1.2% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.174 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.95267 | Mse:0.75129 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 29 out of 100000 Trials\n",
      "512# | Cost: Mae:0.95209 | Mse:0.74576 | ~ | Actions: 2.1% | 2.0% | 1.6% | 1.0% | 1.6% | 85.5% | 2.1% | 1.6% | 2.5% | ~ | Score: 44.0 |  HighScore: 308.0 | ~ | Eps: 0.166 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 30 out of 100000 Trials\n",
      "256# | Cost: Mae:0.96143 | Mse:0.75431 | ~ | Actions: 1.2% | 2.7% | 2.3% | 1.6% | 1.6% | 86.7% | 0.8% | 1.2% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.159 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 31 out of 100000 Trials\n",
      "256# | Cost: Mae:0.95749 | Mse:0.75836 | ~ | Actions: 2.0% | 1.6% | 3.5% | 0.8% | 0.4% | 89.1% | 0.8% | 0.4% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.152 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.96423 | Mse:0.75710 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 32 out of 100000 Trials\n",
      "256# | Cost: Mae:0.97687 | Mse:0.76317 | ~ | Actions: 0.8% | 1.2% | 0.4% | 1.6% | 0.8% | 90.6% | 1.6% | 1.6% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.145 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 33 out of 100000 Trials\n",
      "256# | Cost: Mae:0.96793 | Mse:0.75908 | ~ | Actions: 1.2% | 2.0% | 0.8% | 0.8% | 2.0% | 87.9% | 1.2% | 2.3% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.138 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 34 out of 100000 Trials\n",
      "256# | Cost: Mae:0.98106 | Mse:0.78068 | ~ | Actions: 0.4% | 0.8% | 1.2% | 1.2% | 2.0% | 90.6% | 0.8% | 1.2% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.132 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.97988 | Mse:0.77641 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 35 out of 100000 Trials\n",
      "256# | Cost: Mae:0.97747 | Mse:0.77214 | ~ | Actions: 1.6% | 2.0% | 1.2% | 0.4% | 3.1% | 87.1% | 2.7% | 1.6% | 0.4% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.126 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 36 out of 100000 Trials\n",
      "256# | Cost: Mae:0.97121 | Mse:0.77567 | ~ | Actions: 2.0% | 1.6% | 1.2% | 1.6% | 0.8% | 87.5% | 2.7% | 1.2% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.121 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 37 out of 100000 Trials\n",
      "1280# | Cost: Mae:0.98949 | Mse:0.78115 | ~ | Actions: 1.2% | 1.5% | 0.9% | 1.4% | 1.3% | 89.5% | 1.4% | 1.7% | 0.9% | ~ | Score: 176.0 |  HighScore: 308.0 | ~ | Eps: 0.115 Lives Left: 2 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:1.00279 | Mse:0.78829 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 38 out of 100000 Trials\n",
      "256# | Cost: Mae:0.98158 | Mse:0.78321 | ~ | Actions: 1.2% | 1.2% | 2.3% | 2.3% | 1.6% | 86.3% | 2.0% | 1.2% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.110 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 39 out of 100000 Trials\n",
      "512# | Cost: Mae:0.98866 | Mse:0.78730 | ~ | Actions: 1.0% | 1.6% | 1.8% | 1.0% | 1.2% | 89.3% | 2.1% | 1.2% | 1.0% | ~ | Score: 44.0 |  HighScore: 308.0 | ~ | Eps: 0.105 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 40 out of 100000 Trials\n",
      "256# | Cost: Mae:1.00122 | Mse:0.79230 | ~ | Actions: 1.6% | 0.4% | 1.2% | 1.6% | 1.2% | 91.4% | 2.0% | 0.8% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.101 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:0.98771 | Mse:0.78487 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 41 out of 100000 Trials\n",
      "256# | Cost: Mae:0.98432 | Mse:0.78349 | ~ | Actions: 0.4% | 1.2% | 0.4% | 0.8% | 0.8% | 90.6% | 2.0% | 2.0% | 2.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.096 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 42 out of 100000 Trials\n",
      "256# | Cost: Mae:0.99730 | Mse:0.78816 | ~ | Actions: 0.8% | 2.0% | 0.0% | 0.8% | 0.8% | 93.4% | 0.4% | 0.4% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.092 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 43 out of 100000 Trials\n",
      "256# | Cost: Mae:0.99104 | Mse:0.79388 | ~ | Actions: 2.0% | 0.4% | 0.4% | 0.8% | 0.8% | 91.0% | 1.2% | 2.0% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.088 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:1.00166 | Mse:0.79780 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 44 out of 100000 Trials\n",
      "256# | Cost: Mae:0.99487 | Mse:0.78800 | ~ | Actions: 1.6% | 0.8% | 2.0% | 0.8% | 0.4% | 89.8% | 2.0% | 1.2% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.084 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 45 out of 100000 Trials\n",
      "256# | Cost: Mae:1.00338 | Mse:0.79896 | ~ | Actions: 0.0% | 0.4% | 0.8% | 0.0% | 0.0% | 96.9% | 0.8% | 0.8% | 0.4% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.080 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 46 out of 100000 Trials\n",
      "256# | Cost: Mae:1.00259 | Mse:0.79641 | ~ | Actions: 0.4% | 0.4% | 1.2% | 1.2% | 0.4% | 93.0% | 0.8% | 1.2% | 1.6% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.077 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:1.00231 | Mse:0.79956 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 47 out of 100000 Trials\n",
      "768# | Cost: Mae:1.00123 | Mse:0.79366 | ~ | Actions: 0.4% | 0.8% | 1.0% | 0.9% | 0.3% | 93.2% | 1.3% | 1.4% | 0.7% | ~ | Score: 88.0 |  HighScore: 308.0 | ~ | Eps: 0.073 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 48 out of 100000 Trials\n",
      "256# | Cost: Mae:1.00556 | Mse:0.80024 | ~ | Actions: 0.8% | 0.8% | 0.8% | 0.4% | 0.8% | 94.5% | 0.0% | 0.8% | 1.2% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.070 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 49 out of 100000 Trials\n",
      "256# | Cost: Mae:0.98800 | Mse:0.78651 | ~ | Actions: 0.8% | 2.0% | 0.0% | 0.4% | 0.4% | 94.5% | 1.2% | 0.4% | 0.4% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.067 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:1.00661 | Mse:0.79999 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 50 out of 100000 Trials\n",
      "256# | Cost: Mae:0.99615 | Mse:0.79643 | ~ | Actions: 0.8% | 1.6% | 0.0% | 0.4% | 0.8% | 95.3% | 0.8% | 0.4% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.064 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 51 out of 100000 Trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256# | Cost: Mae:0.99766 | Mse:0.79434 | ~ | Actions: 0.8% | 0.4% | 0.8% | 1.6% | 0.4% | 92.2% | 1.2% | 2.0% | 0.8% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.061 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 52 out of 100000 Trials\n",
      "256# | Cost: Mae:0.98717 | Mse:0.79216 | ~ | Actions: 0.8% | 0.0% | 0.8% | 0.0% | 0.8% | 95.3% | 0.8% | 0.4% | 1.2% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.059 Lives Left: 3 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "256# | Cost: Mae:1.00600 | Mse:0.80532 | ~ | Actions: 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 100.0% | 0.0% | 0.0% | 0.0% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 3 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 53 out of 100000 Trials\n",
      "256# | Cost: Mae:0.99758 | Mse:0.80033 | ~ | Actions: 1.2% | 0.4% | 0.8% | 0.0% | 0.4% | 95.7% | 0.8% | 0.4% | 0.4% | ~ | Score: 0.0 |  HighScore: 308.0 | ~ | Eps: 0.056 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 54 out of 100000 Trials\n",
      "768# | Cost: Mae:0.98043 | Mse:0.79407 | ~ | Actions: 0.7% | 0.8% | 0.9% | 0.5% | 1.2% | 0.4% | 0.4% | 94.3% | 0.9% | ~ | Score: 132.0 |  HighScore: 308.0 | ~ | Eps: 0.053 Lives Left: 3 \t\n",
      "\n",
      "Practing... On Trail 55 out of 100000 Trials\n",
      "256# | Cost: Mae:0.98338 | Mse:0.79376 | ~ | Actions: 1.2% | 0.4% | 0.8% | 0.0% | 1.6% | 0.8% | 1.6% | 92.6% | 1.2% | ~ | Score: 44.0 |  HighScore: 308.0 | ~ | Eps: 0.051 Lives Left: 3 \t\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-6569a8e7e374>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mae\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpratice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-130-125bc8808381>\u001b[0m in \u001b[0;36mpratice\u001b[1;34m(self, trials, testing_interval)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn_trial\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nPracting... On Trail %d out of %d Trials\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_trial\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn_trial\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mtesting_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-125bc8808381>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[1;31m# Training on Experience from the Current (self.Experience) and Previous Game (memory).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetExperienciesFromMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber_Of_Memories_To_Replay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeStep\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[1;31m# STOP PLAYING : If the agent has no lives or not imporving it's score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-125bc8808381>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experience)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\knstr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2619\u001b[0m     \"\"\"\n\u001b[0;32m   2620\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2621\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\knstr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    model=CONV_MODEL,\n",
    "    batch_size =256,\n",
    "    gamma=0.99,\n",
    "    optimizer=Adam(decay=0.99999999),\n",
    "    eps_init=0.59,\n",
    "    number_of_memories_to_replay = 300,\n",
    "    eps_decay=0.9557,\n",
    "    eps_min=0.05,\n",
    "    render_game=False,\n",
    "    max_timesteps = 13000,\n",
    "    metrics=[\"mse\",\"mae\"]\n",
    ")\n",
    "agent.pratice(trials=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Experience[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots();\n",
    "for i in range(100):\n",
    "    img = (memory[0][i][0][0]*255.0).reshape(16,8)\n",
    "    \n",
    "    fig.add_subplot(2,1,2)\n",
    "    ax = fig.imshow(img)\n",
    "    plt.pause(0.1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import ArtistAnimation,FFMpegWriter\n",
    "\n",
    "fig = plt.figure();\n",
    "images = [[plt.imshow((memory[0][i][0][0]*255.0).reshape(16,8), animated=True)] for i in range(100)]\n",
    "\n",
    "animation = ArtistAnimation(fig,images,blit=True,interval=50,repeat_delay=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation.save(\"ram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "animation.save(\"ram.mp4\", writer=writer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "html = None\n",
    "with open(\"./ram.html\") as file:\n",
    "    html = HTML(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(agent.Experience[-1][INFO_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in MongoClient(\"localhost\")[\"Machine_Learning\"][\"Memory\"].find():\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Football', 'admin', 'config', 'local']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MongoClient(\"localhost\").list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,BatchNormalization,Conv1D,Flatten\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Actions: 9\n",
      "Observation Space: (128,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BeamRider-ram-v0\");\n",
    "Number_of_Actions = env.action_space.n\n",
    "Observation_Shape = env.observation_space.shape\n",
    "memory = deque(maxlen=1024)\n",
    "print(\"Number of Actions: {}\\nObservation Space: {}\".format(Number_of_Actions,Observation_Shape));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_INDEX = 0\n",
    "ACT_INDEX = 1\n",
    "XOBS_INDEX = 2\n",
    "REWARD_INDEX = 3\n",
    "DONE_INDEX = 4\n",
    "INFO_INDEX = 5\n",
    "\n",
    "\n",
    "DENSE_MODEL = 0\n",
    "CONV_MODEL = 1\n",
    "\n",
    "DataBase = MongoClient(\"localhost\")[\"Machine_Learning\"]\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self,model=None,model_type=None,loss=\"mse\",optimizer=\"adam\",metrics=[\"mse\",\"mae\"],\n",
    "                 batch_size =1024,number_of_memories_to_replay = 10,gamma=0.99,eps_init=0.9,eps_decay=0.99,\n",
    "                 eps_min=0.05,render_game=True):\n",
    "        \n",
    "        \"\"\"\n",
    "            Eposide_Experience = An Array holding the memory of the eposide;\n",
    "            L          =        Lenght of  Eposide_Experience \n",
    "            N          =        batch_size or self.BatchSize\n",
    "\n",
    "            While playing, After N Timesteps the Agent will train from Eposide_Experience[L: N],\n",
    "        \"\"\"\n",
    "        self.ID = 0\n",
    "        #Agent's Hyper-Parameters\n",
    "        self.Loss = loss\n",
    "        self.Optimizer = optimizer\n",
    "        self.BatchSize = batch_size;\n",
    "        self.Number_Of_Memories_To_Replay = number_of_memories_to_replay;\n",
    "        \n",
    "        #Agent Bellman Paramenters\n",
    "        self.Epilison = eps_init;\n",
    "        self.Epilison_Decay = eps_decay;\n",
    "        self.Epilison_MinLimit = eps_min;\n",
    "        self.Gamma = gamma;\n",
    "        \n",
    "        \n",
    "        # Agent's Settings;\n",
    "        self.Experience = []\n",
    "        self.HighScore = 0;\n",
    "        self.RenderGame = render_game\n",
    "        self.Model_Type = model_type\n",
    "        self.NumberofActions = Number_of_Actions;\n",
    "        \n",
    "        # Agent's History\n",
    "        self.ActionsHistory = []\n",
    "        self.RewardsHistory = []\n",
    "        self.Metrics = list(set(metrics))\n",
    "        self.Metrics.remove(self.Loss);\n",
    "        \n",
    "        self.MetricsHistory = {metric:[] for metric in self.Metrics}\n",
    "        self.MetricsHistory[self.Loss] = []\n",
    "        \n",
    "       \n",
    "        self.formatObservation = lambda obs: np.expand_dims(np.array(obs,dtype=\"float\").reshape(self.Input_Shape)/255.0,axis=0);\n",
    "        self.obsToImg = lambda obs:  np.array(obs,dtype=\"float\").reshape(16,8,1)/255.0\n",
    "        \n",
    "         # Agent's Model;\n",
    "        self.Model = model\n",
    "        if self.Model == None:\n",
    "            self.createDenseNet();\n",
    "        elif self.Model == DENSE_MODEL:\n",
    "            self.createDenseNet();\n",
    "        elif self.Model == CONV_MODEL:\n",
    "            self.createConvNet();\n",
    "            \n",
    "        self.Memory = DataBase[\"Memory\"]\n",
    "            \n",
    "        pass;\n",
    "    def createDenseNet(self):\n",
    "        \n",
    "        \"\"\"\n",
    "            Four Dense Layers - Input Shape \n",
    "        \"\"\"\n",
    "        self.Input_Shape = (1,128);self.Model_Type = DENSE_MODEL;\n",
    "        \n",
    "        self.Model = Sequential();\n",
    "        self.Model.add(Dense(256,use_bias=True,input_shape=self.Input_Shape));\n",
    "        self.Model.add(Activation(\"relu\"));\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.Model.add(Dense(1024,use_bias=True));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"))\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.finalizeModel(\"4 Layers Dense\");\n",
    "    \n",
    "    def createConvNet(self,):\n",
    "        \"\"\"\n",
    "            A Convoluational Neural Network\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Input_Shape = (8,16);self.Model_Type = CONV_MODEL;\n",
    "                \n",
    "        self.Model = Sequential();\n",
    "        self.Model.add(Conv1D(filters=12,kernel_size(4,4),use_bias=True,input_shape=(self.Input_Shape)));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"));\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.Model.add(Conv1D(,4,use_bias=True));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"));\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.Model.add(Flatten());\n",
    "        self.finalizeModel(\"2 Convoluational Layers and 2 Dense Layers\");\n",
    "    \n",
    "    def finalizeModel(self,structure_info):\n",
    "        self.Model.add(Dense(128,use_bias=True));\n",
    "        self.Model.add(BatchNormalization())\n",
    "        self.Model.add(Activation(\"relu\"))\n",
    "        self.Model.add(Dropout(0.25));\n",
    "        \n",
    "        self.Model.add(Dense(self.NumberofActions,use_bias=True,));\n",
    "        self.Model.add(Activation(\"linear\"));\n",
    "        self.Model.compile(loss=self.Loss,optimizer=\"adam\",metrics=self.Metrics);\n",
    "        print(\"Succesfully Created a Neural Network with {} .It has  Input Shape of {}\".format(structure_info,self.Input_Shape),end=\"\\r\")\n",
    "\n",
    "        \n",
    "    def predictAction(self,obs):        \n",
    "        return np.argmax(self.Model.predict(obs))\n",
    "        \n",
    "    def getAction(self,obs):\n",
    "        \"\"\" Choose Random Action if random number is smaller than epilision for exporation \"\"\"\n",
    "        if np.random.uniform(0,1) > self.Epilison :\n",
    "            return self.predictAction(obs);\n",
    "        return np.random.choice(self.NumberofActions); \n",
    "    \n",
    "    def learn(self,experience):\n",
    "        if len(experience) == 0 :\n",
    "            return np.nan\n",
    "        x = np.zeros(shape=((len(experience),)+self.Input_Shape))\n",
    "        y = np.zeros(shape=((len(experience),self.NumberofActions)));\n",
    "        for i,(obs,act,_obs,reward,done,info) in enumerate(experience):\n",
    "            x[i] = obs;\n",
    "            y[i] = self.Model.predict(obs);\n",
    "            #Bellman Equation\n",
    "            y[i:act] = reward;\n",
    "            if not done:\n",
    "                 y[i:act] += reward + self.Gamma * np.max(self.Model.predict(_obs));\n",
    "        \n",
    "        \n",
    "        for metric,cost in zip(self.MetricsHistory,self.Model.train_on_batch(x,y)):\n",
    "            self.MetricsHistory[metric][-1].append(cost)\n",
    "        return self.MetricsHistory[self.Loss][-1][-1]\n",
    "    \n",
    "    def restartGame(self,):\n",
    "        self.TimeStep = 0;\n",
    "        for metric in self.MetricsHistory:\n",
    "            self.MetricsHistory[metric].append([])\n",
    "        self.RewardsHistory.append([])\n",
    "        self.ActionsHistory.append({act:0 for act in range(self.NumberofActions)});\n",
    "        self.Experience = [[self.formatObservation(env.reset())]]\n",
    "        \n",
    "    \n",
    "    def getCurrentScore(self):\n",
    "        score = sum(self.RewardsHistory[-1]);\n",
    "        if score > self.HighScore:self.HighScore = score;\n",
    "        return score\n",
    "    \n",
    "    def parseScore(self):\n",
    "        return \"Score: \"+str(self.getCurrentScore())+\" |  HighScore: \"+str(self.HighScore);\n",
    "    \n",
    "    def parseActions(self,nth_experience=-1):\n",
    "        total = sum([act for act in self.ActionsHistory[nth_experience].values()]);\n",
    "        return \"Actions: \"+\" | \".join([\"%.1f%s\"%(100*(value/total),\"%\") for value in self.ActionsHistory[nth_experience].values()]);\n",
    "    \n",
    "    def parseAverageCost(self,_index=-1,):\n",
    "        return \"Cost: \"+\" | \".join([\"%s:%.5f\"%(metric[0].upper()+metric[1:],sum(self.MetricsHistory[metric][_index])/len(self.MetricsHistory[metric][_index])) for metric in self.MetricsHistory])\n",
    "    \n",
    "    def parseStatus(self):\n",
    "        return \"%d# | %s | ~ | %s | ~ | %s | ~ | Eps: %.3f Lives Left: %d \\t\"%(\n",
    "            self.TimeStep,self.parseAverageCost(),self.parseActions(),self.parseScore(),self.Epilison,\n",
    "            self.Experience[-1][INFO_INDEX]\n",
    "        )\n",
    "    def learnExperience(self,):\n",
    "        self.learn(self.getExperienciesFromMemory(self.Number_Of_Memories_To_Replay));\n",
    "        self.learn(self.Experience[max(0,self.TimeStep-self.BatchSize):])\n",
    "        print(self.parseStatus(),end=\"\\r\");\n",
    "    \n",
    "    def saveExperience(self,):\n",
    "        self.Memory.insert_one({\"model_type\":model_type,\"game_experience\":self.Experience,\"agent_id\":self.ID})\n",
    "        memory.append(self.Experience);\n",
    "    def playGame(self,):\n",
    "        self.restartGame();\n",
    "        while True:\n",
    "           \n",
    "            #Sending Action to the enviroment\n",
    "            self.Experience[-1].append(self.getAction(self.Experience[-1][OBS_INDEX]));\n",
    "            for item in env.step(self.Experience[-1][ACT_INDEX]):\n",
    "                self.Experience[-1].append(item);\n",
    "            self.Experience[-1][INFO_INDEX] = self.Experience[-1][INFO_INDEX][\"ale.lives\"]\n",
    "            self.Experience[-1][XOBS_INDEX] = self.formatObservation(self.Experience[-1][XOBS_INDEX]);\n",
    "            self.RewardsHistory[-1].append(self.Experience[-1][REWARD_INDEX]);\n",
    "            self.ActionsHistory[-1][self.Experience[-1][ACT_INDEX]] += 1;\n",
    "            self.TimeStep +=1;\n",
    "            \n",
    "            # When agent is killed\n",
    "            if self.Experience[-1][DONE_INDEX]:\n",
    "                self.learnExperience();\n",
    "                    \n",
    "                # When all lifes are finished\n",
    "                if self.Experience[-1][INFO_INDEX] == 0:\n",
    "                    break;\n",
    "            \n",
    "            # Training on Experience\n",
    "            if self.TimeStep%self.BatchSize == 0: \n",
    "                self.learnExperience();\n",
    "            \n",
    "            # Rendering Frame(s)\n",
    "            if self.RenderGame:\n",
    "                env.render();\n",
    "                \n",
    "            # Pushing the Next Observation to Experience for the Next loop\n",
    "            self.Experience.append([self.Experience[-1][XOBS_INDEX]])\n",
    "        \n",
    "        \n",
    "        # Reducing The Epilison to give agent a bigger probability to choose action and action\n",
    "        if self.Epilison > self.Epilison_MinLimit:self.Epilison = self.Epilison*self.Epilison_Decay;\n",
    "                \n",
    "    \n",
    "    def getBatchFromExperience(self,experience):\n",
    "        startIndex = max(0,np.random.choice(len(experience))-self.BatchSize);\n",
    "        return experience[startIndex:startIndex+self.BatchSize]\n",
    "    \n",
    "    def getExperienciesFromMemory(self,n_memories=1):\n",
    "        if len(memory) == 0: return []\n",
    "        memories = []\n",
    "        for _ in range(min(len(memory),n_memories)):\n",
    "            memories.extend(self.getBatchFromExperience(memory[np.random.choice(len(memory))]))\n",
    "        return memories\n",
    "                     \n",
    "   \n",
    "        \n",
    "        \n",
    "    def pratice(self,trials=1000,test=2):\n",
    "        np.random.seed(0);\n",
    "        for n_trial in range(0,trials):\n",
    "            print(\"\\nPracting... On Trail %d out of %d Trials\"%(n_trial+1,trials))\n",
    "            self.playGame();\n",
    "            print()\n",
    "           # if n_trial%test == 0:\n",
    "            self.play();\n",
    "                \n",
    "    \n",
    "    \n",
    "    def play(self,):\n",
    "        eps = self.Epilison;\n",
    "        self.Epilison = 0\n",
    "        print(\"Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\")\n",
    "        self.playGame();\n",
    "        self.Epilison = eps;\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully Created a Neural Network with 2 Convoluational Layers and 2 Dense Layers\n",
      "Practing... On Trail 1 out of 100000 Trials\n",
      "1359# | Cost: Mae:0.63537 | Mse:0.59172 | ~ | Actions: 8.7% | 8.8% | 8.8% | 10.3% | 11.4% | 10.1% | 16.3% | 17.4% | 8.3% | ~ | Score: 308.0 |  HighScore: 308.0 | ~ | Eps: 0.790 Lives Left: 0 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "2550# | Cost: Mae:0.36201 | Mse:0.44421 | ~ | Actions: 49.6% | 7.9% | 4.5% | 0.0% | 34.2% | 0.2% | 2.0% | 1.5% | 0.0% | ~ | Score: 176.0 |  HighScore: 308.0 | ~ | Eps: 0.000 Lives Left: 0 \t\n",
      "\n",
      "\n",
      "Practing... On Trail 2 out of 100000 Trials\n",
      "2026# | Cost: Mae:0.26855 | Mse:0.38287 | ~ | Actions: 16.2% | 11.7% | 17.1% | 8.3% | 9.3% | 9.5% | 8.8% | 8.6% | 10.4% | ~ | Score: 352.0 |  HighScore: 352.0 | ~ | Eps: 0.766 Lives Left: 0 \t\n",
      "Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\n",
      "512# | Cost: Mae:1.08408 | Mse:0.37319 | ~ | Actions: 12.1% | 17.8% | 41.4% | 6.6% | 0.0% | 0.0% | 0.0% | 3.9% | 18.2% | ~ | Score: 176.0 |  HighScore: 352.0 | ~ | Eps: 0.000 Lives Left: 2 \t\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c483ac737a92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mae\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpratice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-58fc64982eac>\u001b[0m in \u001b[0;36mpratice\u001b[1;34m(self, trials, test)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m            \u001b[1;31m# if n_trial%test == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-58fc64982eac>\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEpilison\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Playing Game Without Any Random Actions (Epilison == 0). No Training Wheels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEpilison\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-58fc64982eac>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;31m#Sending Action to the enviroment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOBS_INDEX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mACT_INDEX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mINFO_INDEX\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mINFO_INDEX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ale.lives\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\knstr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\knstr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\knstr\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    model=CONV_MODEL,\n",
    "    batch_size =256,\n",
    "    gamma=0.99,\n",
    "    eps_init=0.79,\n",
    "    eps_decay=0.97,\n",
    "    eps_min=0.05,\n",
    "    render_game=True,\n",
    "    metrics=[\"mse\",\"mae\"]\n",
    ")\n",
    "agent.pratice(trials=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots();\n",
    "for i in range(100):\n",
    "    img = (memory[0][i][0][0]*255.0).reshape(16,8)\n",
    "    \n",
    "    fig.add_subplot(2,1,2)\n",
    "    ax = fig.imshow(img)\n",
    "    plt.pause(0.1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import ArtistAnimation,FFMpegWriter\n",
    "\n",
    "fig = plt.figure();\n",
    "images = [[plt.imshow((memory[0][i][0][0]*255.0).reshape(16,8), animated=True)] for i in range(100)]\n",
    "\n",
    "animation = ArtistAnimation(fig,images,blit=True,interval=50,repeat_delay=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation.save(\"ram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "animation.save(\"ram.mp4\", writer=writer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "html = None\n",
    "with open(\"./ram.html\") as file:\n",
    "    html = HTML(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(agent.Experience[-1][INFO_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
